{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wenhan_Liu_Assignment3_Questions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZDadr9_31O3"
      },
      "source": [
        "Q1.\n",
        "\n",
        "In Machine Learning, bias is the error in model assumptions, difference between the expected prediction of the model and the correct value. It's how much the average model over all training sets differs from the true model. When the bias is too large, the model would be inaccurate, as there's not enough flexibility. It's kind of the accuracy of the model. Variance is how much models estimated from different training sets differ from each other, when the variance is too large, model would be inaccurate as there's too much sensitivity to the sample, the model may perform well during the training state, but will perform poorly with test sets. It's kind of the precision of the model.\n",
        "\n",
        "Underfitting happens with the model that is too simple to represent all the relevant class characteristics, it's relationship with bias and vairance is that it could happen when there's high bias and low variance, and for models with underfitting, the training and testing error are both very high. Overfitting happens with the model that is too complex and fits irrelevant characteristics in the data, its' relationship with bias and vairance is that is could happen when there's low bias and high variance, and for models with overfitting, the training error could be low, but the training error would be really high"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ5ToZsNCXAt"
      },
      "source": [
        "Q2.\n",
        "\n",
        "When given a linear classifier but the data is not linearly separable, we can map it to a higher-dimensional space, which means is to map the original input space to some higher-dimensional feature space where the training set is separable. As for the kernel trick, it kind of provides a shortcut, instead of explicitly computing the lifting transformation φ(x) that map the input space to a higher-dimensional feature space, we can define a kernel function K(xi,xj), which is a dot product in some feature space that gives a non-linear dicision boundary in the original feature space. And a kernel function is a function that can be applied to pairs of input examples to evaluate dot products in some corresponding feature space, so in this way, we do not need to compute φ explicitly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd-jMXpnD3EZ"
      },
      "source": [
        "Q3.\n",
        "\n",
        "\n",
        "```\n",
        "# Inputs\n",
        "# train_feats : N x d matrix of N features each d descriptor long\n",
        "# train_labels : N x 1 array containing values of either -1 ( class 0)\n",
        "or 1 ( class 1)\n",
        "# test_feat : 1 x d image for which we wish to predict a label\n",
        "# Outputs\n",
        "# -1 ( class 0) or 1 ( class 1)\n",
        "#\n",
        "# Please turn this into a multi - class classifier for k classes .\n",
        "# Inputs : As before , except\n",
        "# train_labels : N x 1 array of class label integers from 0 to k-1\n",
        "# Outputs :\n",
        "# A class label integer from 0 to k-1\n",
        "#\n",
        "\n",
        "#I used one vs all approach for this problem.\n",
        "\n",
        "\n",
        "def classify ( train_feats , train_labels , test_feat )\n",
        "  result = np.zeros((N, 1))\n",
        "  for i in range(k)：\n",
        "    new_train_labels = train_labels\n",
        "    new_train_labels[new_train_labels != i] = -1 #So the new training label contains i and -1 where i is label for one class\n",
        "\n",
        "    # Train classification hyperplane\n",
        "    weights , bias = train_linear_classifier ( train_feats , new_train_label )\n",
        "\n",
        "    # Compute distance from hyperplane\n",
        "    test_score = weights * test_feats + bias\n",
        "    result[i] = test_score\n",
        "  return np.argmax(result) #This would return the index of the largest element in result, which is also the label for the class\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4UzG2vLQOKu"
      },
      "source": [
        "Q4.\n",
        "\n",
        "Because for this problem, we are using K-means clustering for classification, and for K-means, we randomly select K centers, and assign each point to nearest center. Since many features from SIFT are almost equidistant, it would be hard for the algorithm to assign each point to the nearest center, and this could cause no change during the re-assignment step, hence cause classification problems. So if want to improve the classification accuracy, maybe could use consine similarity instead of Euclidean distance, as it could ignore the scale and equidistant may not have much influence on it. And another way maybe to let the algorithm only finishes after it reaches maximum number of iterations, in this way, maybe the slight difference in the distance would cause a little influence during each iteration, and eventually would let the K centers to converge into the right place."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYd0VtlVX-EV"
      },
      "source": [
        "Q5.\n",
        "\n",
        "For Bag of Words models, some advantages would be that they are flexible to geometry/deformations/viewpoint. Say, if the training set is sufficiently representative, the codebook would be universal. So the model can learn on a dataset, and be applied on many other datasets. For example, the model can be trained with scenes in Italy, and when tested with scenes in Canada, it would still have pretty good performance. On the other hand, the background and foreground are mixed when the bag covers the hwole image, also, the models would ignore geometry. With disadvantages like these, if given three images with one showing steppe, another one showing gradual green, and the last one being a pure green image full of noises, then their histograms would look very similar. And if the bag of words model is based on histograms, then it would classify these three images as the same scene, even though they are clearly not, and a way to overcome this problem is with the use of spatial pyramid."
      ]
    }
  ]
}